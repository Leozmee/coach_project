{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Your input_length: 561 is bigger than 0.9 * max_length: 256. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m     q_fr = translator(clean_html(qbody))[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtranslation_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ans \u001b[38;5;129;01min\u001b[39;00m answers_map[qid][:\u001b[32m2\u001b[39m]:   \u001b[38;5;66;03m# max 2 rÃ©ponses/textes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         a_fr = \u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtranslation_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     55\u001b[39m         pairs.append((q_fr, a_fr))\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# 4. Ã‰chantillonnage final (â‰¤Â 6â€¯000 paires), mÃ©lange alÃ©atoire\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:405\u001b[39m, in \u001b[36mTranslationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[33;03m    Translate the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    378\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m \u001b[33;03m          token ids of the translation.\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:186\u001b[39m, in \u001b[36mText2TextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    158\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[33;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[32m    160\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    188\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    189\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[32m0\u001b[39m])\n\u001b[32m    190\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[32m    191\u001b[39m     ):\n\u001b[32m    192\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1431\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1424\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1425\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1428\u001b[39m         )\n\u001b[32m   1429\u001b[39m     )\n\u001b[32m   1430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1438\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1437\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1439\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1338\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1337\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:215\u001b[39m, in \u001b[36mText2TextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    213\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m out_b = output_ids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2412\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2408\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[32m   2411\u001b[39m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2412\u001b[39m     model_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[32m   2417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:854\u001b[39m, in \u001b[36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[39m\u001b[34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[39m\n\u001b[32m    852\u001b[39m encoder_kwargs[\u001b[33m\"\u001b[39m\u001b[33mreturn_dict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    853\u001b[39m encoder_kwargs[model_input_name] = inputs_tensor\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m]: ModelOutput = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/models/marian/modeling_marian.py:732\u001b[39m, in \u001b[36mMarianEncoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     inputs_embeds = \u001b[38;5;28mself\u001b[39m.embed_tokens(input_ids) * \u001b[38;5;28mself\u001b[39m.embed_scale\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m embed_pos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m hidden_states = inputs_embeds + embed_pos\n\u001b[32m    735\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/models/marian/modeling_marian.py:108\u001b[39m, in \u001b[36mMarianSinusoidalPositionalEmbedding.forward\u001b[39m\u001b[34m(self, input_ids_shape, past_key_values_length, position_ids)\u001b[39m\n\u001b[32m    104\u001b[39m     bsz, seq_len = input_ids_shape[:\u001b[32m2\u001b[39m]\n\u001b[32m    105\u001b[39m     position_ids = torch.arange(\n\u001b[32m    106\u001b[39m         past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=\u001b[38;5;28mself\u001b[39m.weight.device\n\u001b[32m    107\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Fineâ€‘tuning **mT5â€‘small** (Seq2Seq) â€“ ChatbotÂ Fitness ðŸ‡«ðŸ‡·\n",
    "# CiblÃ© pour **RTXâ€¯3060â€¯Laptop (6â€¯GoÂ VRAM) + 16â€¯GoÂ RAM** â†’ â‰ˆÂ 1â€¯h dâ€™entraÃ®nement\n",
    "\n",
    "# 1. Imports & setup\n",
    "import os, json, random, torch, xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig, GenerationConfig, pipeline\n",
    ")\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Helper nettoyage HTML\n",
    "def clean_html(html_str):\n",
    "    return BeautifulSoup(html_str, \"html.parser\").get_text(\" \", strip=True)\n",
    "\n",
    "# 2. Dataset fitness JSON ðŸ‡«ðŸ‡·\n",
    "DATA_PATH = \"/home/maxime/DataDevIA/chatbotcoach_project/data/fitness/coach_sportif_dataset.json\"\n",
    "raw = json.load(open(DATA_PATH, \"r\", encoding=\"utf-8\"))\n",
    "pairs = list({(d[\"input\"].strip(), d[\"output\"].strip()) for d in raw[\"conversations\"]})\n",
    "\n",
    "# 3. Enrichir avec StackExchange (XML EN) + traduction limitÃ©e (â‰¤Â 2â€¯000 QA)\n",
    "xml_path = \"/home/maxime/DataDevIA/chatbotcoach_project/data/stackexchange/Posts.xml\"\n",
    "answers_map = defaultdict(list)\n",
    "for _, elem in ET.iterparse(xml_path, events=(\"end\",)):\n",
    "    if elem.tag == \"row\" and elem.get(\"PostTypeId\") == \"2\":\n",
    "        answers_map[elem.get(\"ParentId\")].append(elem.get(\"Body\"))\n",
    "    elem.clear()\n",
    "\n",
    "questions = {}\n",
    "for _, elem in ET.iterparse(xml_path, events=(\"end\",)):\n",
    "    if elem.tag == \"row\" and elem.get(\"PostTypeId\") == \"1\" and elem.get(\"Id\") in answers_map:\n",
    "        questions[elem.get(\"Id\")] = elem.get(\"Body\")\n",
    "    elem.clear()\n",
    "\n",
    "# ðŸ‘‰ On garde la VRAM GPU libre pour le fineâ€‘tuningÂ : traduction sur CPU\n",
    "translator = pipeline(\n",
    "    \"translation_en_to_fr\",\n",
    "    model=\"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    device=-1,            # CPU pour Ã©viter les erreurs CUDA\n",
    "    batch_size=16,\n",
    "    max_length=256,\n",
    ")\n",
    "MAX_XML_QA = 2000  # limite stricte pour tenir <Â 1â€¯h\n",
    "for idx, (qid, qbody) in enumerate(questions.items()):\n",
    "    if idx >= MAX_XML_QA:\n",
    "        break\n",
    "    q_fr = translator(clean_html(qbody))[0][\"translation_text\"]\n",
    "    for ans in answers_map[qid][:2]:   # max 2 rÃ©ponses/textes\n",
    "        a_fr = translator(clean_html(ans))[0][\"translation_text\"]\n",
    "        pairs.append((q_fr, a_fr))\n",
    "\n",
    "# 4. Ã‰chantillonnage final (â‰¤Â 6â€¯000 paires), mÃ©lange alÃ©atoire\n",
    "random.shuffle(pairs)\n",
    "MAX_PAIRS = 6000\n",
    "pairs = pairs[:MAX_PAIRS]\n",
    "print(f\"Paires conservÃ©esÂ : {len(pairs)}\")\n",
    "\n",
    "# 5. Tokenizer mT5\n",
    "tok = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "# 6. Dataset HFÂ : question â†’ rÃ©ponse\n",
    "src, tgt = zip(*pairs)\n",
    "ds = Dataset.from_dict({\"src\": src, \"tgt\": tgt}).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = tok(batch[\"src\"], truncation=True, max_length=MAX_LEN)\n",
    "    with tok.as_target_tokenizer():\n",
    "        labels = tok(batch[\"tgt\"], truncation=True, max_length=MAX_LEN)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "ds = ds.map(preprocess, batched=True, remove_columns=[\"src\", \"tgt\"], num_proc=4)\n",
    "\n",
    "# 7. Data collator & modÃ¨le 4â€‘bit NF4\n",
    "collator = DataCollatorForSeq2Seq(tok, model=None)\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\", quantization_config=bnb_cfg, device_map=\"auto\")\n",
    "\n",
    "# 8. EntraÃ®nement (â‰ˆÂ 1â€¯h)\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"coach-mt5-small\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,   # batch effectif 8\n",
    "    num_train_epochs=4,              # < 1Â h sur 3060\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_LEN,\n",
    "    generation_num_beams=2,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 9. Sauvegarde\n",
    "out_dir = \"coach-mt5s-final\"\n",
    "model.save_pretrained(out_dir)\n",
    "tok.save_pretrained(out_dir)\n",
    "print(f\"ModÃ¨le sauvegardÃ© â†’ {out_dir}\")\n",
    "\n",
    "# 10. Inference helper\n",
    "cfg = GenerationConfig(max_new_tokens=100, num_beams=2, length_penalty=1.0)\n",
    "\n",
    "def chat_mt5(query: str):\n",
    "    encoded = tok(query, return_tensors=\"pt\").to(model.device)\n",
    "    ids = model.generate(**encoded, generation_config=cfg)\n",
    "    return tok.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Tests d'infÃ©rence\n",
    "print(chat_mt5(\"Comment amÃ©liorer mon endurance pour la course Ã  pied ?\"))\n",
    "print(chat_mt5(\"Quel programme pour perdre du poids en 3 mois ?\"))\n",
    "print(chat_mt5(\"Quels Ã©tirements faire aprÃ¨s une sÃ©ance de squat ?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
