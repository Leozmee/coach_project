{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ dataset FR rechargé : data/stackexchange/translated_dataset_fr\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch, os, tqdm\n",
    "\n",
    "CACHE = Path(\"data/stackexchange/translated_dataset_fr\")\n",
    "if CACHE.exists():\n",
    "    dataset_tr = load_from_disk(CACHE)\n",
    "    print(\"✓ dataset FR rechargé :\", CACHE)\n",
    "else:\n",
    "    print(\"→ traduction en cours… (~10 min sur RTX 3060)\")\n",
    "    tok_mt = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "    mod_mt = (AutoModelForSeq2SeqLM\n",
    "              .from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "              .to(\"cuda\", dtype=torch.float16))\n",
    "\n",
    "    def translate(texts, max_len=128):\n",
    "        with torch.no_grad():\n",
    "            enc = tok_mt(texts, return_tensors=\"pt\",\n",
    "                         padding=True, truncation=True,\n",
    "                         max_length=max_len).to(\"cuda\")\n",
    "            out = mod_mt.generate(**enc, max_length=max_len,\n",
    "                                  num_beams=4, early_stopping=True)\n",
    "        return tok_mt.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "    def pipeline(batch):\n",
    "        batch[\"q_fr\"] = translate(batch[\"q_en\"])\n",
    "        batch[\"a_fr\"] = translate(batch[\"a_en\"])\n",
    "        return batch\n",
    "\n",
    "    raw_ds = Dataset.from_pandas(posts_df)\n",
    "    dataset_tr = raw_ds.map(pipeline, batched=True, batch_size=64,\n",
    "                            remove_columns=[\"q_en\", \"a_en\"])\n",
    "    dataset_tr.save_to_disk(CACHE)\n",
    "    print(\"✓ dataset FR sauvegardé :\", CACHE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3855 train | 429 val | 477 test\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "ds = load_from_disk(\"data/stackexchange/translated_dataset_fr\")\n",
    "\n",
    "tmp = ds.train_test_split(test_size=0.1, seed=42)\n",
    "test_ds = tmp[\"test\"]\n",
    "tmp2 = tmp[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_ds, val_ds = tmp2[\"train\"], tmp2[\"test\"]\n",
    "\n",
    "print(len(train_ds), \"train |\", len(val_ds), \"val |\", len(test_ds), \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# 1) Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "# 2) Charger en 8-bit\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/mt5-small\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 3) Configurer LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "# 4) Enrober le modèle\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f0adc371504e38b2efbdf60fe18d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9f1bb9702d4228a818e3dbe0a95328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/429 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e0266decd041c681795e90272503d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MT5Tokenizer\n",
    "\n",
    "tok = MT5Tokenizer.from_pretrained(\"google/mt5-small\", model_max_length=512)\n",
    "\n",
    "MAX_IN, MAX_OUT = 128, 64\n",
    "def preprocess(batch):\n",
    "    src = [\"question: \" + q for q in batch[\"q_fr\"]]\n",
    "    tgt = [\"answer: \"   + a for a in batch[\"a_fr\"]]\n",
    "    model_in  = tok(src, padding=\"max_length\", truncation=True,\n",
    "                    max_length=MAX_IN)\n",
    "    model_out = tok(tgt, padding=\"max_length\", truncation=True,\n",
    "                    max_length=MAX_OUT)\n",
    "    model_in[\"labels\"] = model_out[\"input_ids\"]\n",
    "    return model_in\n",
    "\n",
    "train_ds = train_ds.map(preprocess, batched=True, remove_columns=[\"q_fr\",\"a_fr\"])\n",
    "val_ds   = val_ds.map(preprocess,   batched=True, remove_columns=[\"q_fr\",\"a_fr\"])\n",
    "test_ds  = test_ds.map(preprocess,  batched=True, remove_columns=[\"q_fr\",\"a_fr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "≈ 481 pas par époque\n"
     ]
    }
   ],
   "source": [
    "# ─── Cellule 4.5 – Calcul du nombre de pas par époque ───\n",
    "# batch_size = 1  et gradient_accumulation_steps = 8\n",
    "steps_per_epoch = len(train_ds) // (1 * 8)\n",
    "print(\"≈\", steps_per_epoch, \"pas par époque\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perplexity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     16\u001b[39m args = TrainingArguments(\n\u001b[32m     17\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33mmt5_fitness_ckpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     per_device_train_batch_size=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     report_to=[],\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 5) Instancier le Trainer (sans passer predict_with_generate ici)\u001b[39;00m\n\u001b[32m     31\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     32\u001b[39m     model=model,\n\u001b[32m     33\u001b[39m     args=args,\n\u001b[32m     34\u001b[39m     train_dataset=train_ds,\n\u001b[32m     35\u001b[39m     eval_dataset=val_ds,\n\u001b[32m     36\u001b[39m     data_collator=collator,\n\u001b[32m     37\u001b[39m     tokenizer=tok,\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     compute_metrics=\u001b[43mperplexity\u001b[49m,   \u001b[38;5;66;03m# votre fn qui fera generate→perplexity\u001b[39;00m\n\u001b[32m     39\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# 6) Lancer l’entraînement\u001b[39;00m\n\u001b[32m     42\u001b[39m trainer.train()\n",
      "\u001b[31mNameError\u001b[39m: name 'perplexity' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 1) Charger & configurer le modèle\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2) Vider la mémoire CUDA\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 3) Collator\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tok, model=model, label_pad_token_id=-100)\n",
    "\n",
    "# 4) Arguments d’entraînement (sans predict_with_generate ni generation_max_length)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mt5_fitness_ckpt\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    eval_steps=steps_per_epoch,\n",
    "    save_steps=steps_per_epoch,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "# 5) Instancier le Trainer (sans passer predict_with_generate ici)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=perplexity,   # votre fn qui fera generate→perplexity\n",
    ")\n",
    "\n",
    "# 6) Lancer l’entraînement\n",
    "trainer.train()\n",
    "\n",
    "# 7) Pour évaluer avec génération :\n",
    "#    soit en une passe brute :\n",
    "metrics = trainer.evaluate()\n",
    "#    soit en generation explicite :\n",
    "preds = trainer.predict(test_ds, max_length=MAX_OUT, num_beams=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "save_dir = Path(\"mt5_fitness_final\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 7-1  poids LoRA seulement (2 × ~30 Mo)\n",
    "model.save_pretrained(save_dir, safe_serialization=True)\n",
    "tok.save_pretrained(save_dir)\n",
    "print(\"✓ modèle LoRA sauvegardé :\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "ckpt = \"mt5_fitness_final\"\n",
    "tok = MT5Tokenizer.from_pretrained(ckpt)\n",
    "base = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\",\n",
    "                                                   torch_dtype=torch.float16).to(\"cuda\")\n",
    "model = PeftModel.from_pretrained(base, ckpt).to(\"cuda\").eval()\n",
    "\n",
    "def ask(q, max_new=64):\n",
    "    prompt = \"question: \" + q\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(**enc, max_length=max_new,\n",
    "                             num_beams=4,\n",
    "                             temperature=0.7,\n",
    "                             decoder_start_token_id=tok.pad_token_id)\n",
    "    return tok.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "print(ask(\"Comment améliorer mon endurance pour la course à pied ?\"))\n",
    "print(\"----\")\n",
    "print(ask(\"Quels étirements faire après une séance de squat ?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
