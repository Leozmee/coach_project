{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # Chatbot Fitness (Français) – Notebook de Fine-Tuning\n",
    "#\n",
    "# Ce notebook vous guide pas-à-pas pour :\n",
    "#\n",
    "# 1. Télécharger/assembler un petit jeu de données Q-R sur le fitness en français.  \n",
    "# 2. Pré-traiter et (optionnellement) traduire des données anglophones.  \n",
    "# 3. Fine-tuner **`dbddv01/gpt2-french-small`** à l’aide de LoRA + 8-bit sur GPU ≤ 6 Go VRAM.  \n",
    "# 4. Sauvegarder et tester le modèle sous forme de chatbot.\n",
    "#\n",
    "# > ⚠️ Matériel testé : RTX 3060 Laptop 6 Go VRAM + Ryzen 7 5800H + 16 Go RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %%\n",
    "# Installation des dépendances principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers datasets accelerate peft bitsandbytes sentencepiece kaggle sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## (Facultatif) Configurer l’API Kaggle\n",
    "#\n",
    "# Si vous utilisez ce notebook en local/Colab et souhaitez télécharger le jeu *Fitness AI Prompt-Completion Dataset* depuis Kaggle :\n",
    "#\n",
    "# ```bash\n",
    "# mkdir -p ~/.kaggle\n",
    "# echo '{\"username\":\"___\",\"key\":\"___\"}' > ~/.kaggle/kaggle.json\n",
    "# chmod 600 ~/.kaggle/kaggle.json\n",
    "# kaggle datasets download -d chibss/fitness-ai-prompt-completion-dataset -p data/ \\\n",
    "#        && unzip data/fitness-ai-prompt-completion-dataset.zip -d data/\n",
    "# ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Loading illuin/fquad requires you to execute the dataset script in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Dataset 1 : FQuAD v1.1 (FR)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m fquad = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43milluin/fquad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplain_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Dataset 2 : Fitness Q-R anglophone (Kaggle) – optionnel\u001b[39;00m\n\u001b[32m      9\u001b[39m fit_path = \u001b[33m\"\u001b[39m\u001b[33mdata/fitaidataset.json\u001b[39m\u001b[33m\"\u001b[39m          \u001b[38;5;66;03m# adapte le chemin si nécessaire\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/datasets/load.py:2062\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2057\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2058\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2059\u001b[39m )\n\u001b[32m   2061\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/datasets/load.py:1782\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1780\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1781\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1795\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/datasets/load.py:1664\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1659\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1660\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1661\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1662\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1663\u001b[39m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1665\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m trust_remote_code:\n\u001b[32m   1666\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1667\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or any data file in the same directory.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1668\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/datasets/load.py:1614\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1605\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1606\u001b[39m     \u001b[38;5;66;03m# Otherwise we must use the dataset script if the user trusts it\u001b[39;00m\n\u001b[32m   1607\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[32m   1616\u001b[39m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision != \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/datasets/load.py:1277\u001b[39m, in \u001b[36mHubDatasetModuleFactoryWithScript.get_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1266\u001b[39m         _create_importable_file(\n\u001b[32m   1267\u001b[39m             local_path=local_path,\n\u001b[32m   1268\u001b[39m             local_imports=local_imports,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1274\u001b[39m             download_mode=\u001b[38;5;28mself\u001b[39m.download_mode,\n\u001b[32m   1275\u001b[39m         )\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1277\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1278\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires you to execute the dataset script in that\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1279\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1280\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m         )\n\u001b[32m   1282\u001b[39m _check_library_imports(name=\u001b[38;5;28mself\u001b[39m.name, library_imports=library_imports)\n\u001b[32m   1283\u001b[39m module_path, \u001b[38;5;28mhash\u001b[39m = _load_importable_file(\n\u001b[32m   1284\u001b[39m     dynamic_modules_path=dynamic_modules_path,\n\u001b[32m   1285\u001b[39m     module_namespace=\u001b[33m\"\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1286\u001b[39m     subdirectory_name=\u001b[38;5;28mhash\u001b[39m,\n\u001b[32m   1287\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m   1288\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Loading illuin/fquad requires you to execute the dataset script in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "# Télécharger / charger les jeux de données\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, DatasetDict\n",
    "import json, os, pandas as pd\n",
    "\n",
    "# Dataset 1 : FQuAD v1.1 (FR)\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# Dataset 1 : FQuAD v1.1 (FR)\n",
    "# => on active `trust_remote_code` pour autoriser le script\n",
    "fquad = load_dataset(\n",
    "    \"illuin/fquad\",\n",
    "    \"plain_text\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Dataset 2 : Fitness Q-R anglophone (Kaggle) – optionnel\n",
    "#fit_path = \"data/fitaidataset.json\"          # adapte le chemin si nécessaire\n",
    "#fitness_ds = None\n",
    "if os.path.isfile(fit_path):\n",
    "    with open(fit_path, \"r\", encoding=\"utf8\") as f:\n",
    "        raw = json.load(f)                   # attend des champs 'prompt', 'completion'\n",
    "    df = pd.DataFrame(raw)\n",
    "    fitness_ds = Dataset.from_pandas(df)     # simple split 'train'\n",
    "\n",
    "# Concaténer ou fallback\n",
    "dataset = fquad\n",
    "if fitness_ds:\n",
    "    dataset = concatenate_datasets([fquad, fitness_ds])\n",
    "\n",
    "print(dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optionnel) Traduire le jeu Fitness EN→FR\n",
    "from transformers import pipeline\n",
    "\n",
    "if fitness_ds:\n",
    "    translator = pipeline(\n",
    "        \"translation_en_to_fr\",\n",
    "        model=\"Helsinki-NLP/opus-mt-en-fr\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    def translate_example(ex):\n",
    "        # ex comporte 'prompt' / 'completion'\n",
    "        ex[\"prompt_fr\"] = translator(ex[\"prompt\"])[0][\"translation_text\"]\n",
    "        ex[\"completion_fr\"] = translator(ex[\"completion\"])[0][\"translation_text\"]\n",
    "        return ex\n",
    "\n",
    "    fitness_ds = fitness_ds.map(translate_example, batched=False)\n",
    "    # Remplace la version EN par FR\n",
    "    dataset = concatenate_datasets([fquad, fitness_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise en forme « conversation »\n",
    "def make_convo(ex):\n",
    "    if \"question\" in ex:                                   # FQuAD\n",
    "        ex[\"text\"] = (\n",
    "            f\"Utilisateur: {ex['question']}\\n\"\n",
    "            f\"Assistant: {ex['answers']['text'][0]}\"\n",
    "        )\n",
    "    elif \"prompt_fr\" in ex:                                # Fitness traduit\n",
    "        ex[\"text\"] = (\n",
    "            f\"Utilisateur: {ex['prompt_fr']}\\n\"\n",
    "            f\"Assistant: {ex['completion_fr']}\"\n",
    "        )\n",
    "    return ex\n",
    "\n",
    "dataset = dataset.map(make_convo, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbddv01/gpt2-french-small\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Préparation LoRA + 8-bit et entraînement\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"dbddv01/gpt2-french-small\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-fitness-fr\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"no\"\n",
    ")\n",
    "\n",
    "def data_collator(features):\n",
    "    return {\n",
    "        \"input_ids\":  torch.stack([f[\"input_ids\"] for f in features]),\n",
    "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "        \"labels\": torch.stack([f[\"input_ids\"] for f in features]),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"gpt2-fitness-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test rapide du chatbot\n",
    "from transformers import pipeline\n",
    "\n",
    "chat = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2-fitness-fr\",\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "prompt = \"Utilisateur: Comment améliorer mon endurance ?\\nAssistant:\"\n",
    "print(\n",
    "    chat(prompt, max_new_tokens=80, do_sample=True, top_p=0.9, temperature=0.8)[0][\"generated_text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Impossible de trouver data/fquad depuis /home",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m     root = cur.parent\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImpossible de trouver data/fquad depuis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcur\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m os.chdir(root)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRacine détectée :\u001b[39m\u001b[33m\"\u001b[39m, root)\n",
      "\u001b[31mRuntimeError\u001b[39m: Impossible de trouver data/fquad depuis /home"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Chatbot Fitness – Fine-Tuning GPT-2 French\n",
    "#\n",
    "# Ce notebook vous guide, étape par étape, pour :\n",
    "# 1. Repérer la racine du projet (quel que soit le cwd initial).  \n",
    "# 2. Installer les dépendances.  \n",
    "# 3. Charger FQuAD depuis `data/fquad/train.json`.  \n",
    "# 4. Extraire les paires Q/A depuis `data/stackexchange/Posts.xml`.  \n",
    "# 5. Formater en conversation “Utilisateur / Assistant”.  \n",
    "# 6. Tokeniser avec GPT-2 FR.  \n",
    "# 7. Fine-tuner en 8-bit + LoRA.  \n",
    "# 8. Tester votre chatbot.\n",
    "\n",
    "# %%\n",
    "# 1. Détection robuste de la racine du projet\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "cur = Path().resolve()\n",
    "if cur.name == \"notebook\":\n",
    "    root = cur.parent\n",
    "elif (cur / \"data\" / \"fquad\").exists():\n",
    "    root = cur\n",
    "elif (cur.parent / \"data\" / \"fquad\").exists():\n",
    "    root = cur.parent\n",
    "else:\n",
    "    raise RuntimeError(f\"Impossible de trouver data/fquad depuis {cur}\")\n",
    "\n",
    "os.chdir(root)\n",
    "print(\"Racine détectée :\", root)\n",
    "print(\"Contenu racine    :\", list(root.iterdir()))\n",
    "\n",
    "# %%\n",
    "# 2. Installer les dépendances\n",
    "import sys, subprocess\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\",\n",
    "    \"transformers\", \"datasets\", \"accelerate\",\n",
    "    \"peft\", \"bitsandbytes\", \"sentencepiece\", \"sacrebleu\"\n",
    "])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Charger FQuAD (train.json)\n",
    "#  \n",
    "# Assurez-vous d’avoir dézippé `download-form-fquad1.0.zip` dans `data/fquad/`.\n",
    "\n",
    "# %%\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "fquad_dir = root / \"data\" / \"fquad\"\n",
    "print(\"Contenu data/fquad:\", list(fquad_dir.iterdir()))\n",
    "\n",
    "with open(fquad_dir / \"train.json\", encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "qa_list = []\n",
    "for art in raw[\"data\"]:\n",
    "    for para in art[\"paragraphs\"]:\n",
    "        for qa in para[\"qas\"]:\n",
    "            if qa.get(\"answers\"):\n",
    "                qa_list.append({\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answer\":   qa[\"answers\"][0][\"text\"]\n",
    "                })\n",
    "\n",
    "fquad = Dataset.from_list(qa_list)\n",
    "print(f\"✅ FQuAD chargé : {len(fquad)} exemples\")\n",
    "print(fquad[0])\n",
    "\n",
    "# %%\n",
    "# 4. Extraire paires Q/A depuis StackExchange\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "stack_dir = root / \"data\" / \"stackexchange\"\n",
    "print(\"Contenu data/stackexchange:\", list(stack_dir.iterdir()))\n",
    "\n",
    "xml_path = stack_dir / \"Posts.xml\"\n",
    "\n",
    "# 4.1 Map {AcceptedAnswerId → question_body}\n",
    "qmap = {}\n",
    "for _, elem in ET.iterparse(xml_path, events=(\"end\",)):\n",
    "    if elem.tag == \"row\" and elem.attrib.get(\"PostTypeId\") == \"1\":\n",
    "        acc = elem.attrib.get(\"AcceptedAnswerId\")\n",
    "        if acc:\n",
    "            qmap[acc] = elem.attrib.get(\"Body\", \"\")\n",
    "    elem.clear()\n",
    "\n",
    "# 4.2 Construire paires prompt/completion\n",
    "pairs = []\n",
    "for _, elem in ET.iterparse(xml_path, events=(\"end\",)):\n",
    "    if elem.tag == \"row\" and elem.attrib.get(\"PostTypeId\") == \"2\":\n",
    "        aid = elem.attrib.get(\"Id\")\n",
    "        if aid in qmap:\n",
    "            pairs.append({\n",
    "                \"prompt\":     qmap[aid],\n",
    "                \"completion\": elem.attrib.get(\"Body\", \"\")\n",
    "            })\n",
    "    elem.clear()\n",
    "\n",
    "# 4.3 Nettoyage HTML\n",
    "def clean_html(text):\n",
    "    return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "for p in pairs:\n",
    "    p[\"prompt\"]     = clean_html(p[\"prompt\"])\n",
    "    p[\"completion\"] = clean_html(p[\"completion\"])\n",
    "\n",
    "# 4.4 Conversion en Dataset HF\n",
    "df = pd.DataFrame(pairs)\n",
    "fitness_ds = Dataset.from_pandas(df)\n",
    "print(f\"✅ Extrait {len(fitness_ds)} paires Q/A fitness\")\n",
    "print(fitness_ds[0])\n",
    "\n",
    "# %%\n",
    "# 5. Formater en conversation “Utilisateur / Assistant”\n",
    "def to_convo(ex, inp, out):\n",
    "    ex[\"text\"] = f\"Utilisateur: {ex[inp]}\\nAssistant: {ex[out]}\"\n",
    "    return ex\n",
    "\n",
    "fquad_convo = fquad.map(\n",
    "    lambda ex: to_convo(ex, \"question\", \"answer\"),\n",
    "    remove_columns=[c for c in fquad.column_names if c != \"text\"]\n",
    ")\n",
    "fitness_convo = fitness_ds.map(\n",
    "    lambda ex: to_convo(ex, \"prompt\", \"completion\"),\n",
    "    remove_columns=[\"prompt\", \"completion\"]\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "dataset = concatenate_datasets([fquad_convo, fitness_convo])\n",
    "print(f\"✅ Dataset total : {len(dataset)} exemples\")\n",
    "print(dataset[0][\"text\"])\n",
    "\n",
    "# %%\n",
    "# 6. Tokenisation pour GPT-2 FR\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbddv01/gpt2-french-small\")\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    tokenize_fn, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "print(\"✅ Tokenisation OK — colonnes :\", tokenized.column_names)\n",
    "\n",
    "# %%\n",
    "# 7. Fine-tuning (8-bit + LoRA)\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"dbddv01/gpt2-french-small\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(root / \"gpt2-fitness-fr\"),\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"no\"\n",
    ")\n",
    "\n",
    "def data_collator(features):\n",
    "    return {\n",
    "        \"input_ids\":      torch.stack([f[\"input_ids\"] for f in features]),\n",
    "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "        \"labels\":         torch.stack([f[\"input_ids\"] for f in features]),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(root / \"gpt2-fitness-fr\")\n",
    "\n",
    "# %%\n",
    "# 8. Test rapide du chatbot\n",
    "from transformers import pipeline\n",
    "\n",
    "chat = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=str(root / \"gpt2-fitness-fr\"),\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "prompt = \"Utilisateur: Quels exercices pour les épaules ?\\nAssistant:\"\n",
    "out = chat(prompt, max_new_tokens=60, do_sample=True, top_p=0.9, temperature=0.8)\n",
    "print(out[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD initial : /home/maxime/DataDevIA/chatbotcoach_project/notebook\n",
      "CWD final   : /home/maxime/DataDevIA/chatbotcoach_project\n",
      "→ data/fquad existe : True\n",
      "→ Posts.xml existe  : True\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: peft in ./.venv/lib/python3.12/site-packages (0.15.2)\n",
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.12/site-packages (0.46.0)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacrebleu in ./.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.12/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.venv/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.12/site-packages (from sacrebleu) (5.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FQuAD chargé : 20731 exemples\n",
      "{'question': \"Quel astronome a émit l'idée en premier d'une planète entre les orbites de Mars et Jupiter ?\", 'answer': 'Johann Elert Bode'}\n",
      "✅ Extrait 4761 paires Q/A fitness\n",
      "{'prompt': \"What's the difference? I'm looking at shake options and some contain whey isolate, some contain whey concentrate and some both.\\n\", 'completion': 'The main difference is in the \"purity\", how much lactose and fat is left with the protein after filtering. Whey isolate usually contains around 90% protein and whey concentrate is more like 70-85%.\\n\\nIf you have trouble digesting the lactose or are trying to minimize carbohydrate content, then whey isolate would be a good choice. Otherwise, it probably doesn\\'t matter; just pick the concentrate since it\\'s cheaper in terms of protein grams/dollar. \\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20731/20731 [00:00<00:00, 28329.46 examples/s]\n",
      "Map: 100%|██████████| 4761/4761 [00:00<00:00, 27118.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset total : 25492 exemples\n",
      "Utilisateur: Quel astronome a émit l'idée en premier d'une planète entre les orbites de Mars et Jupiter ?\n",
      "Assistant: Johann Elert Bode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25492/25492 [00:01<00:00, 15398.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenisation OK — colonnes : ['input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 166\u001b[39m\n\u001b[32m    160\u001b[39m lora_cfg = LoraConfig(\n\u001b[32m    161\u001b[39m     r=\u001b[32m8\u001b[39m, lora_alpha=\u001b[32m16\u001b[39m, lora_dropout=\u001b[32m0.1\u001b[39m,\n\u001b[32m    162\u001b[39m     target_modules=[\u001b[33m\"\u001b[39m\u001b[33mc_attn\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mc_proj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mc_fc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    163\u001b[39m )\n\u001b[32m    164\u001b[39m model = get_peft_model(model, lora_cfg)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt2-fitness-fr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    177\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdata_collator\u001b[39m(features):\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    181\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m:      torch.stack([f[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]),\n\u001b[32m    182\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m: torch.stack([f[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]),\n\u001b[32m    183\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m:         torch.stack([f[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]),\n\u001b[32m    184\u001b[39m     }\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Chatbot Fitness – Fine-Tuning GPT-2 French\n",
    "#\n",
    "# Ce notebook vous guide, étape par étape, pour :\n",
    "# 1. Vérifier et positionner le répertoire de travail (cwd).  \n",
    "# 2. Installer les dépendances.  \n",
    "# 3. Charger FQuAD depuis `data/fquad/train.json`.  \n",
    "# 4. Extraire les paires Q/A depuis `data/stackexchange/Posts.xml`.  \n",
    "# 5. Formater en conversation “Utilisateur / Assistant”.  \n",
    "# 6. Tokeniser avec GPT-2 FR.  \n",
    "# 7. Fine-tuner en 8-bit + LoRA.  \n",
    "# 8. Tester votre chatbot.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "\n",
    "# 1. Si on travaille depuis le dossier 'notebook', on remonte d'un niveau\n",
    "print(\"CWD initial :\", os.getcwd())\n",
    "if os.path.basename(os.getcwd()) == \"notebook\":\n",
    "    os.chdir(\"..\")\n",
    "print(\"CWD final   :\", os.getcwd())\n",
    "\n",
    "# Chemins relatifs simples\n",
    "fquad_dir = \"data/fquad\"\n",
    "posts_xml = \"data/stackexchange/Posts.xml\"\n",
    "\n",
    "# Vérification rapide\n",
    "print(\"→ data/fquad existe :\", os.path.isdir(fquad_dir))\n",
    "print(\"→ Posts.xml existe  :\", os.path.isfile(posts_xml))\n",
    "\n",
    "# %%\n",
    "# 2. Installer les dépendances (exécuter une fois)\n",
    "import sys, subprocess\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\",\n",
    "    \"transformers\", \"datasets\", \"accelerate\",\n",
    "    \"peft\", \"bitsandbytes\", \"sentencepiece\", \"sacrebleu\"\n",
    "])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Charger FQuAD manuellement  \n",
    "# Assurez-vous d’avoir dézippé `download-form-fquad1.0.zip` dans `data/fquad/` (fichiers `train.json` et `valid.json`).\n",
    "\n",
    "# %%\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(os.path.join(fquad_dir, \"train.json\"), encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "qa_list = []\n",
    "for art in raw[\"data\"]:\n",
    "    for para in art[\"paragraphs\"]:\n",
    "        for qa in para[\"qas\"]:\n",
    "            if qa.get(\"answers\"):\n",
    "                qa_list.append({\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answer\":   qa[\"answers\"][0][\"text\"]\n",
    "                })\n",
    "\n",
    "fquad = Dataset.from_list(qa_list)\n",
    "print(f\"✅ FQuAD chargé : {len(fquad)} exemples\")\n",
    "print(fquad[0])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Extraire les paires Q/A depuis StackExchange\n",
    "\n",
    "# %%\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "# 4.1 Construire le mapping question → réponse acceptée\n",
    "qmap = {}\n",
    "for _, elem in ET.iterparse(posts_xml, events=(\"end\",)):\n",
    "    if elem.tag == \"row\" and elem.attrib.get(\"PostTypeId\") == \"1\":\n",
    "        acc = elem.attrib.get(\"AcceptedAnswerId\")\n",
    "        if acc:\n",
    "            qmap[acc] = elem.attrib.get(\"Body\", \"\")\n",
    "    elem.clear()\n",
    "\n",
    "# 4.2 Recuperer les paires prompt/completion\n",
    "pairs = []\n",
    "for _, elem in ET.iterparse(posts_xml, events=(\"end\",)):\n",
    "    if elem.tag == \"row\" and elem.attrib.get(\"PostTypeId\") == \"2\":\n",
    "        aid = elem.attrib.get(\"Id\")\n",
    "        if aid in qmap:\n",
    "            pairs.append({\n",
    "                \"prompt\":     qmap[aid],\n",
    "                \"completion\": elem.attrib.get(\"Body\", \"\")\n",
    "            })\n",
    "    elem.clear()\n",
    "\n",
    "# 4.3 Nettoyage HTML très simple\n",
    "def clean_html(text):\n",
    "    return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "for p in pairs:\n",
    "    p[\"prompt\"]     = clean_html(p[\"prompt\"])\n",
    "    p[\"completion\"] = clean_html(p[\"completion\"])\n",
    "\n",
    "# 4.4 Conversion en Dataset HF\n",
    "df = pd.DataFrame(pairs)\n",
    "fitness_ds = Dataset.from_pandas(df)\n",
    "print(f\"✅ Extrait {len(fitness_ds)} paires Q/A fitness\")\n",
    "print(fitness_ds[0])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Formater en conversation “Utilisateur / Assistant”\n",
    "\n",
    "# %%\n",
    "def to_convo(ex, inp, out):\n",
    "    ex[\"text\"] = f\"Utilisateur: {ex[inp]}\\nAssistant: {ex[out]}\"\n",
    "    return ex\n",
    "\n",
    "fquad_convo = fquad.map(\n",
    "    lambda ex: to_convo(ex, \"question\", \"answer\"),\n",
    "    remove_columns=[c for c in fquad.column_names if c != \"text\"]\n",
    ")\n",
    "fitness_convo = fitness_ds.map(\n",
    "    lambda ex: to_convo(ex, \"prompt\", \"completion\"),\n",
    "    remove_columns=[\"prompt\", \"completion\"]\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "dataset = concatenate_datasets([fquad_convo, fitness_convo])\n",
    "print(f\"✅ Dataset total : {len(dataset)} exemples\")\n",
    "print(dataset[0][\"text\"])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Tokenisation pour GPT-2 FR\n",
    "\n",
    "# %%\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbddv01/gpt2-french-small\")\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    tokenize_fn, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "print(\"✅ Tokenisation OK — colonnes :\", tokenized.column_names)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Fine-tuning (8-bit + LoRA)\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"dbddv01/gpt2-french-small\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-fitness-fr\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"no\"\n",
    ")\n",
    "\n",
    "def data_collator(features):\n",
    "    return {\n",
    "        \"input_ids\":      torch.stack([f[\"input_ids\"] for f in features]),\n",
    "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "        \"labels\":         torch.stack([f[\"input_ids\"] for f in features]),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"gpt2-fitness-fr\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Test rapide du chatbot\n",
    "\n",
    "# %%\n",
    "from transformers import pipeline\n",
    "\n",
    "chat = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2-fitness-fr\",\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "prompt = \"Utilisateur: Quels exercices pour les épaules ?\\nAssistant:\"\n",
    "out = chat(prompt, max_new_tokens=60, do_sample=True, top_p=0.9, temperature=0.8)\n",
    "print(out[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD initial : /home/maxime/DataDevIA/chatbotcoach_project\n",
      "CWD final   : /home/maxime/DataDevIA/chatbotcoach_project\n",
      "data/fquad       exists: True\n",
      "data/stackexchange exists: True\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: peft in ./.venv/lib/python3.12/site-packages (0.15.2)\n",
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.12/site-packages (0.46.0)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacrebleu in ./.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.12/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.venv/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.12/site-packages (from sacrebleu) (5.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "✅ FQuAD chargé : 20731 exemples\n",
      "{'question': \"Quel astronome a émit l'idée en premier d'une planète entre les orbites de Mars et Jupiter ?\", 'answer': 'Johann Elert Bode'}\n",
      "✅ Extrait 4761 paires Q/A fitness\n",
      "{'prompt': \"What's the difference? I'm looking at shake options and some contain whey isolate, some contain whey concentrate and some both.\\n\", 'completion': 'The main difference is in the \"purity\", how much lactose and fat is left with the protein after filtering. Whey isolate usually contains around 90% protein and whey concentrate is more like 70-85%.\\n\\nIf you have trouble digesting the lactose or are trying to minimize carbohydrate content, then whey isolate would be a good choice. Otherwise, it probably doesn\\'t matter; just pick the concentrate since it\\'s cheaper in terms of protein grams/dollar. \\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20731/20731 [00:00<00:00, 31616.58 examples/s]\n",
      "Map: 100%|██████████| 4761/4761 [00:00<00:00, 26510.60 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset total : 25492 exemples\n",
      "Utilisateur: Quel astronome a émit l'idée en premier d'une planète entre les orbites de Mars et Jupiter ?\n",
      "Assistant: Johann Elert Bode\n",
      "→ 22942 exemples train, 2550 exemples eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22942/22942 [00:01<00:00, 14155.67 examples/s]\n",
      "Map: 100%|██████████| 2550/2550 [00:00<00:00, 15577.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenisation terminée\n",
      "  train cols: ['input_ids', 'attention_mask']\n",
      "   eval cols: ['input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 213\u001b[39m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    200\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m:      torch.stack([f[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]),\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m: torch.stack([f[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]),\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m:         torch.stack([f[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]),\n\u001b[32m    203\u001b[39m     }\n\u001b[32m    205\u001b[39m trainer = Trainer(\n\u001b[32m    206\u001b[39m     model=model,\n\u001b[32m    207\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    210\u001b[39m     data_collator=data_collator\n\u001b[32m    211\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33mgpt2-fitness-fr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# %% 9. Évaluation finale & perplexité\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/trainer.py:2509\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2507\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2508\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2509\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2510\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2511\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/transformers/trainer.py:5263\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5262\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5263\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5264\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5265\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/accelerate/data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 200\u001b[39m, in \u001b[36mdata_collator\u001b[39m\u001b[34m(features)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdata_collator\u001b[39m(features):\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m:      \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m: torch.stack([f[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]),\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m:         torch.stack([f[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]),\n\u001b[32m    203\u001b[39m     }\n",
      "\u001b[31mTypeError\u001b[39m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Chatbot Fitness – Fine-Tuning GPT-2 French avec Évaluation\n",
    "#\n",
    "# Ce notebook vous guide, étape par étape, pour :\n",
    "# 1. Vérifier et positionner le répertoire de travail.  \n",
    "# 2. Installer les dépendances.  \n",
    "# 3. Charger FQuAD depuis `data/fquad/train.json`.  \n",
    "# 4. Extraire les paires Q/A depuis `data/stackexchange/Posts.xml`.  \n",
    "# 5. Formater en conversation “Utilisateur / Assistant”.  \n",
    "# 6. Fractionner en train / eval.  \n",
    "# 7. Tokeniser pour GPT-2 FR.  \n",
    "# 8. Fine-tuner en 8-bit + LoRA avec évaluation par époque.  \n",
    "# 9. Évaluer et calculer la perplexité.  \n",
    "# 10. Test rapide du chatbot.\n",
    "\n",
    "# %%\n",
    "# 1. Positionner le cwd si nécessaire\n",
    "import os\n",
    "\n",
    "print(\"CWD initial :\", os.getcwd())\n",
    "if os.path.basename(os.getcwd()) == \"notebook\":\n",
    "    os.chdir(\"..\")\n",
    "print(\"CWD final   :\", os.getcwd())\n",
    "\n",
    "# Vérification simple des chemins\n",
    "print(\"data/fquad       exists:\", os.path.isdir(\"data/fquad\"))\n",
    "print(\"data/stackexchange exists:\", os.path.isdir(\"data/stackexchange\"))\n",
    "\n",
    "fquad_dir = \"data/fquad\"\n",
    "posts_xml = \"data/stackexchange/Posts.xml\"\n",
    "\n",
    "# %%\n",
    "# 2. Installer les dépendances (à exécuter une fois)\n",
    "import sys, subprocess\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\",\n",
    "    \"transformers\", \"datasets\", \"accelerate\",\n",
    "    \"peft\", \"bitsandbytes\", \"sentencepiece\", \"sacrebleu\"\n",
    "])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Charger FQuAD manuellement  \n",
    "# Ayez préalablement dézippé `download-form-fquad1.0.zip` dans `data/fquad/`  \n",
    "# pour obtenir `train.json` et `valid.json`.\n",
    "\n",
    "# %%\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(os.path.join(fquad_dir, \"train.json\"), encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "qa_list = []\n",
    "for art in raw[\"data\"]:\n",
    "    for para in art[\"paragraphs\"]:\n",
    "        for qa in para[\"qas\"]:\n",
    "            if qa.get(\"answers\"):\n",
    "                qa_list.append({\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answer\":   qa[\"answers\"][0][\"text\"]\n",
    "                })\n",
    "\n",
    "fquad = Dataset.from_list(qa_list)\n",
    "print(f\"✅ FQuAD chargé : {len(fquad)} exemples\")\n",
    "print(fquad[0])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Extraire les paires Q/A depuis StackExchange\n",
    "\n",
    "# %%\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "# 4.1 Map {AcceptedAnswerId → question_body}\n",
    "qmap = {}\n",
    "for _, elem in ET.iterparse(posts_xml, events=(\"end\",)):\n",
    "    if elem.tag == \"row\" and elem.attrib.get(\"PostTypeId\") == \"1\":\n",
    "        acc = elem.attrib.get(\"AcceptedAnswerId\")\n",
    "        if acc:\n",
    "            qmap[acc] = elem.attrib.get(\"Body\", \"\")\n",
    "    elem.clear()\n",
    "\n",
    "# 4.2 Construire paires prompt/completion\n",
    "pairs = []\n",
    "for _, elem in ET.iterparse(posts_xml, events=(\"end\",)):\n",
    "    if elem.tag == \"row\" and elem.attrib.get(\"PostTypeId\") == \"2\":\n",
    "        aid = elem.attrib.get(\"Id\")\n",
    "        if aid in qmap:\n",
    "            pairs.append({\n",
    "                \"prompt\":     qmap[aid],\n",
    "                \"completion\": elem.attrib.get(\"Body\", \"\")\n",
    "            })\n",
    "    elem.clear()\n",
    "\n",
    "# 4.3 Nettoyage HTML\n",
    "def clean_html(text):\n",
    "    return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "for p in pairs:\n",
    "    p[\"prompt\"]     = clean_html(p[\"prompt\"])\n",
    "    p[\"completion\"] = clean_html(p[\"completion\"])\n",
    "\n",
    "# 4.4 Conversion en Dataset HF\n",
    "df = pd.DataFrame(pairs)\n",
    "fitness_ds = Dataset.from_pandas(df)\n",
    "print(f\"✅ Extrait {len(fitness_ds)} paires Q/A fitness\")\n",
    "print(fitness_ds[0])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Formater en conversation “Utilisateur / Assistant”\n",
    "\n",
    "# %%\n",
    "def to_convo(ex, inp, out):\n",
    "    ex[\"text\"] = f\"Utilisateur: {ex[inp]}\\nAssistant: {ex[out]}\"\n",
    "    return ex\n",
    "\n",
    "fquad_convo = fquad.map(\n",
    "    lambda ex: to_convo(ex, \"question\", \"answer\"),\n",
    "    remove_columns=[c for c in fquad.column_names if c != \"text\"]\n",
    ")\n",
    "fitness_convo = fitness_ds.map(\n",
    "    lambda ex: to_convo(ex, \"prompt\", \"completion\"),\n",
    "    remove_columns=[\"prompt\", \"completion\"]\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "dataset = concatenate_datasets([fquad_convo, fitness_convo])\n",
    "print(f\"✅ Dataset total : {len(dataset)} exemples\")\n",
    "print(dataset[0][\"text\"])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Fractionner en train / eval (10% eval)\n",
    "\n",
    "# %%\n",
    "from datasets import DatasetDict\n",
    "\n",
    "splits = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = splits[\"train\"]\n",
    "eval_dataset  = splits[\"test\"]\n",
    "print(f\"→ {len(train_dataset)} exemples train, {len(eval_dataset)} exemples eval\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Tokenisation pour GPT-2 FR\n",
    "\n",
    "# %%\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbddv01/gpt2-french-small\")\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_fn, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_fn, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "print(\"✅ Tokenisation terminée\")\n",
    "print(\"  train cols:\", tokenized_train.column_names)\n",
    "print(\"   eval cols:\", tokenized_eval.column_names)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Fine-tuning (8-bit + LoRA) avec évaluation\n",
    "\n",
    "# %% 8. Fine-tuning (8-bit + LoRA) — version compatible\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"dbddv01/gpt2-french-small\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-fitness-fr\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2         # conserve 2 checkpoints\n",
    "    # (on retire evaluation_strategy / save_strategy\n",
    ")\n",
    "\n",
    "def data_collator(features):\n",
    "    return {\n",
    "        \"input_ids\":      torch.stack([f[\"input_ids\"] for f in features]),\n",
    "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "        \"labels\":         torch.stack([f[\"input_ids\"] for f in features]),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,   # on garde l’éval ; elle sera utilisée plus tard\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"gpt2-fitness-fr\")\n",
    "\n",
    "# %% 9. Évaluation finale & perplexité\n",
    "import math\n",
    "metrics = trainer.evaluate()       # calcule eval_loss sur eval_dataset\n",
    "print(\"Eval metrics:\", metrics)\n",
    "print(\"Perplexité :\", math.exp(metrics[\"eval_loss\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Test rapide du chatbot\n",
    "\n",
    "# %%\n",
    "from transformers import pipeline\n",
    "\n",
    "chat = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2-fitness-fr\",\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "prompt = \"Utilisateur: Quels exercices pour les épaules ?\\nAssistant:\"\n",
    "out = chat(prompt, max_new_tokens=60, do_sample=True, top_p=0.9, temperature=0.8)\n",
    "print(out[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD initial : /home/maxime/DataDevIA/chatbotcoach_project\n",
      "CWD final   : /home/maxime/DataDevIA/chatbotcoach_project\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: peft in ./.venv/lib/python3.12/site-packages (0.15.2)\n",
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.12/site-packages (0.46.0)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacrebleu in ./.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.12/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.venv/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.12/site-packages (from sacrebleu) (5.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "✅ FQuAD chargé : 20731 exemples\n",
      "{'question': \"Quel astronome a émit l'idée en premier d'une planète entre les orbites de Mars et Jupiter ?\", 'answer': 'Johann Elert Bode'}\n",
      "✅ Fitness QA : 4761 paires\n",
      "{'prompt': \"What's the difference? I'm looking at shake options and some contain whey isolate, some contain whey concentrate and some both.\\n\", 'completion': 'The main difference is in the \"purity\", how much lactose and fat is left with the protein after filtering. Whey isolate usually contains around 90% protein and whey concentrate is more like 70-85%.\\n\\nIf you have trouble digesting the lactose or are trying to minimize carbohydrate content, then whey isolate would be a good choice. Otherwise, it probably doesn\\'t matter; just pick the concentrate since it\\'s cheaper in terms of protein grams/dollar. \\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20731/20731 [00:00<00:00, 30773.22 examples/s]\n",
      "Map: 100%|██████████| 4761/4761 [00:00<00:00, 26036.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total exemples : 25492\n",
      "Utilisateur: Quel astronome a émit l'idée en premier d'une planète entre les orbites de Mars et Jupiter ?\n",
      "Assistant: Johann Elert Bode\n",
      "→ Train: 22942, Eval: 2550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22942/22942 [00:01<00:00, 16065.46 examples/s]\n",
      "Map: 100%|██████████| 2550/2550 [00:00<00:00, 15167.96 examples/s]\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenisation terminée\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4302' max='4302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4302/4302 31:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.084100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.833600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.552100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.412700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.441900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>4.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>4.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>4.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>3.996300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>4.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>3.952300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>4.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>3.937600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>4.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>3.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>4.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.916500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.943200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>4.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>4.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>3.902900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.960600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>3.957900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>3.924600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>4.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>4.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.973000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/maxime/DataDevIA/chatbotcoach_project/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='324' max='319' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [319/319 09:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_runtime': 30.1433, 'eval_samples_per_second': 84.596, 'eval_steps_per_second': 10.583, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 187\u001b[39m\n\u001b[32m    185\u001b[39m metrics = trainer.evaluate()\n\u001b[32m    186\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEval metrics:\u001b[39m\u001b[33m\"\u001b[39m, metrics)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPerplexité :\u001b[39m\u001b[33m\"\u001b[39m, math.exp(\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# ## 10. Test rapide du chatbot\u001b[39;00m\n\u001b[32m    191\u001b[39m \n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "\u001b[31mKeyError\u001b[39m: 'eval_loss'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Chatbot Fitness – Fine-Tuning GPT-2 French avec Évaluation\n",
    "#\n",
    "# Ce notebook vous guide, étape par étape, pour :\n",
    "# 1. Positionner le cwd.  \n",
    "# 2. Installer les dépendances.  \n",
    "# 3. Charger FQuAD depuis `data/fquad/train.json`.  \n",
    "# 4. Extraire les paires Q/A depuis `data/stackexchange/Posts.xml`.  \n",
    "# 5. Formater en conversation.  \n",
    "# 6. Fractionner train/éval.  \n",
    "# 7. Tokeniser.  \n",
    "# 8. Fine-tuner en 8-bit + LoRA avec évaluation.  \n",
    "# 9. Évaluer & calculer la perplexité.  \n",
    "# 10. Tester le chatbot.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "\n",
    "print(\"CWD initial :\", os.getcwd())\n",
    "if os.path.basename(os.getcwd()) == \"notebook\":\n",
    "    os.chdir(\"..\")\n",
    "print(\"CWD final   :\", os.getcwd())\n",
    "\n",
    "assert os.path.isdir(\"data/fquad\"), \"⛔ data/fquad introuvable\"\n",
    "assert os.path.isdir(\"data/stackexchange\"), \"⛔ data/stackexchange introuvable\"\n",
    "\n",
    "fquad_dir = \"data/fquad\"\n",
    "posts_xml = \"data/stackexchange/Posts.xml\"\n",
    "\n",
    "# %%\n",
    "# 2. Installer les dépendances (une fois)\n",
    "import sys, subprocess\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\",\n",
    "    \"transformers\", \"datasets\", \"accelerate\",\n",
    "    \"peft\", \"bitsandbytes\", \"sentencepiece\", \"sacrebleu\"\n",
    "])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Charger FQuAD manuellement\n",
    "\n",
    "# %%\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(os.path.join(fquad_dir, \"train.json\"), encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "qa_list = []\n",
    "for art in raw[\"data\"]:\n",
    "    for para in art[\"paragraphs\"]:\n",
    "        for qa in para[\"qas\"]:\n",
    "            if qa.get(\"answers\"):\n",
    "                qa_list.append({\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answer\":   qa[\"answers\"][0][\"text\"]\n",
    "                })\n",
    "\n",
    "fquad = Dataset.from_list(qa_list)\n",
    "print(f\"✅ FQuAD chargé : {len(fquad)} exemples\")\n",
    "print(fquad[0])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Extraire les paires Q/A depuis StackExchange\n",
    "\n",
    "# %%\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "# mapping question → accepted answer ID\n",
    "qmap = {}\n",
    "for _, elem in ET.iterparse(posts_xml, events=(\"end\",)):\n",
    "    if elem.tag==\"row\" and elem.attrib.get(\"PostTypeId\")==\"1\":\n",
    "        acc=elem.attrib.get(\"AcceptedAnswerId\")\n",
    "        if acc: qmap[acc]=elem.attrib.get(\"Body\",\"\")\n",
    "    elem.clear()\n",
    "\n",
    "pairs=[]\n",
    "for _, elem in ET.iterparse(posts_xml, events=(\"end\",)):\n",
    "    if elem.tag==\"row\" and elem.attrib.get(\"PostTypeId\")==\"2\":\n",
    "        aid=elem.attrib.get(\"Id\")\n",
    "        if aid in qmap:\n",
    "            pairs.append({\"prompt\":qmap[aid],\"completion\":elem.attrib.get(\"Body\",\"\")})\n",
    "    elem.clear()\n",
    "\n",
    "def clean_html(t): return re.sub(r\"<[^>]+>\",\"\",t)\n",
    "for p in pairs:\n",
    "    p[\"prompt\"]=clean_html(p[\"prompt\"])\n",
    "    p[\"completion\"]=clean_html(p[\"completion\"])\n",
    "\n",
    "fitness_ds=Dataset.from_pandas(pd.DataFrame(pairs))\n",
    "print(f\"✅ Fitness QA : {len(fitness_ds)} paires\")\n",
    "print(fitness_ds[0])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Formater en conversation\n",
    "\n",
    "# %%\n",
    "def to_convo(ex, inp, out):\n",
    "    ex[\"text\"]=f\"Utilisateur: {ex[inp]}\\nAssistant: {ex[out]}\"\n",
    "    return ex\n",
    "\n",
    "fquad_convo = fquad.map(lambda ex: to_convo(ex,\"question\",\"answer\"), remove_columns=[c for c in fquad.column_names if c!=\"text\"])\n",
    "fitness_convo = fitness_ds.map(lambda ex: to_convo(ex,\"prompt\",\"completion\"), remove_columns=[\"prompt\",\"completion\"])\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "dataset = concatenate_datasets([fquad_convo, fitness_convo])\n",
    "print(f\"✅ Total exemples : {len(dataset)}\")\n",
    "print(dataset[0][\"text\"])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Split train / eval (10%)\n",
    "\n",
    "# %%\n",
    "splits = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = splits[\"train\"]\n",
    "eval_dataset  = splits[\"test\"]\n",
    "print(f\"→ Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Tokenisation pour GPT-2 FR\n",
    "\n",
    "# %%\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbddv01/gpt2-french-small\")\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_eval  = eval_dataset.map(tokenize_fn,  batched=True, remove_columns=[\"text\"])\n",
    "print(\"✅ Tokenisation terminée\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Fine-tuning (8-bit + LoRA) et évaluation\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# modèle + LoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(\"dbddv01/gpt2-french-small\", load_in_8bit=True, device_map=\"auto\")\n",
    "lora_cfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[\"c_attn\",\"c_proj\",\"c_fc\"])\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "# args avec éval par époque\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-fitness-fr\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    # ces deux ne sont pas supportés par ta version : on fera eval manuellement après\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # save_strategy=\"epoch\",\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# collator causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"gpt2-fitness-fr\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Loss moyenne : 3.2777\n",
      "→ Perplexité   : 26.51\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 9. Évaluation manuelle & calcul de la perplexité (sans OOM)\n",
    "\n",
    "# %%\n",
    "import math, torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# 9.1 Créer un DataLoader pour le jeu d'évaluation\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_eval, \n",
    "    batch_size=1,               # mini-batch réduit pour tenir en VRAM\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# 9.2 Passer en mode évaluation\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "count = 0\n",
    "\n",
    "# 9.3 Boucle d'évaluation\n",
    "for batch in eval_dataloader:\n",
    "    # déplacer batch sur le même device que le modèle\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch)\n",
    "    total_loss += out.loss.item()\n",
    "    count += 1\n",
    "\n",
    "# 9.4 Moyenne et perplexité\n",
    "avg_loss = total_loss / count\n",
    "print(f\"→ Loss moyenne : {avg_loss:.4f}\")\n",
    "print(f\"→ Perplexité   : {math.exp(avg_loss):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
