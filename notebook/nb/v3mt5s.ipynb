{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "for name in (\"model\", \"trainer\", \"outputs\", \"batch\"):\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, warnings\n",
    "from datasets import load_from_disk\n",
    "from transformers import EvalPrediction\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def compute_perplexity(pred: EvalPrediction):\n",
    "    # pred.predictions = logits (float16 -> float32 via numpy)\n",
    "    logits, labels = pred.predictions, pred.label_ids\n",
    "    vocab = logits.shape[-1]\n",
    "    import torch.nn.functional as F\n",
    "    loss = F.cross_entropy(\n",
    "        torch.from_numpy(logits).view(-1, vocab),\n",
    "        torch.from_numpy(labels).view(-1),\n",
    "        ignore_index=-100,\n",
    "        reduction=\"mean\",\n",
    "    )\n",
    "    return {\"perplexity\": math.exp(loss)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3855 train | 429 val | 477 test\n"
     ]
    }
   ],
   "source": [
    "ds_path = \"data/stackexchange/translated_dataset_fr\"   # <— votre dossier\n",
    "ds      = load_from_disk(ds_path)\n",
    "\n",
    "tmp     = ds.train_test_split(test_size=0.1, seed=42)\n",
    "test_ds = tmp[\"test\"]\n",
    "tmp2    = tmp[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_ds, val_ds = tmp2[\"train\"], tmp2[\"test\"]\n",
    "\n",
    "print(len(train_ds), \"train |\", len(val_ds), \"val |\", len(test_ds), \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 300,864,896 || trainable%: 0.2287\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    MT5Tokenizer, MT5ForConditionalGeneration, BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "tok = MT5Tokenizer.from_pretrained(\"google/mt5-small\", model_max_length=512)\n",
    "\n",
    "# 3) Charger la base en 8-bit + fp16 sur **UN** GPU\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "base = MT5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/mt5-small\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},      # <= ❗️ un seul GPU, plus de NoneType\n",
    ")\n",
    "base.config.use_cache = False\n",
    "base.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# 3-b : passage en mode k-bit & ajout LoRA\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q\", \"v\"],           # matrices clés/valeurs des attn MT5\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "model.print_trainable_parameters()       # ~6 M params ↗\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_IN, MAX_OUT = 128, 64\n",
    "\n",
    "def preprocess(b):\n",
    "    src = [\"question: \" + q for q in b[\"q_fr\"]]\n",
    "    tgt = [\"answer: \"   + a for a in b[\"a_fr\"]]\n",
    "    model_in  = tok(src, padding=\"max_length\", truncation=True, max_length=MAX_IN)\n",
    "    out       = tok(tgt, padding=\"max_length\", truncation=True, max_length=MAX_OUT)\n",
    "    model_in[\"labels\"] = out[\"input_ids\"]\n",
    "    return model_in\n",
    "\n",
    "train_ds = train_ds.map(preprocess, batched=True, remove_columns=[\"q_fr\", \"a_fr\"])\n",
    "val_ds   = val_ds.map(preprocess,   batched=True, remove_columns=[\"q_fr\", \"a_fr\"])\n",
    "test_ds  = test_ds.map(preprocess,  batched=True, remove_columns=[\"q_fr\", \"a_fr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4931/3929027385.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1928' max='1928' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1928/1928 55:14, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>67280414.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1450947215.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>307461.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>28853550448.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>236657.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>65391.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1553.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>78456340.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>70560461619.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>104313.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3500626411.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1996636.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4261451.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>22910819368.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>110613882.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>269239.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>131.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1626653491.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>481083.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1961459384.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>100763.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1377306.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>22910622.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3649576.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>134384.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>123547822.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>66026752.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>18445056081.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>810424.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>390.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>8148.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>437313413.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1074886901.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>73834112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>40091968.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1348644.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>15696.696300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1928, training_loss=3926859816.551582, metrics={'train_runtime': 3316.7136, 'train_samples_per_second': 4.649, 'train_steps_per_second': 0.581, 'total_flos': 2046479255470080.0, 'train_loss': 3926859816.551582, 'epoch': 4.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = len(train_ds) // 8          # batch=1 × accum=8\n",
    "collator        = DataCollatorForSeq2Seq(tok, model, label_pad_token_id=-100)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"mt5_fitness_ckpt\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=5e-5,\n",
    "    eval_steps=steps_per_epoch,\n",
    "    save_steps=steps_per_epoch,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    report_to=[],                      # désactive wandb / tensorboard\n",
    "    remove_unused_columns=False,       # indispensable avec PEFT\n",
    "    skip_memory_metrics=True,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()               # libère la VRAM avant d’allouer le modèle\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=compute_perplexity,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Comment améliorer mon endurance pour la course à pied ?\n",
      "→ <extra_id_0>.\n",
      "\n",
      "> Quel programme pour perdre du poids en 3 mois ?\n",
      "→ <extra_id_0>.\n",
      "\n",
      "> Quels étirements faire après une séance de squat ?\n",
      "→ <extra_id_0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
    "\n",
    "# 1. Charge ton tokenizer et ton modèle fine-tuné\n",
    "checkpoint = \"mt5_fitness_ckpt\"  # ton dossier de checkpoint\n",
    "tok = MT5Tokenizer.from_pretrained(checkpoint)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(checkpoint).to(\"cuda\")\n",
    "\n",
    "# 2. Prépare une fonction d’inférence\n",
    "def ask(question: str, \n",
    "        max_in: int = 192, \n",
    "        max_out: int = 128, \n",
    "        num_beams: int = 4):\n",
    "    # Prefixe “question:” comme à l’entraînement\n",
    "    inp = tok(\n",
    "        \"question: \" + question,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=max_in\n",
    "    ).to(model.device)\n",
    "    # Génère la réponse\n",
    "    out = model.generate(\n",
    "        **inp,\n",
    "        max_length=max_out,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# 3. Teste quelques questions\n",
    "for q in [\n",
    "    \"Comment améliorer mon endurance pour la course à pied ?\",\n",
    "    \"Quel programme pour perdre du poids en 3 mois ?\",\n",
    "    \"Quels étirements faire après une séance de squat ?\"\n",
    "]:\n",
    "    print(f\"> {q}\\n→ {ask(q)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
