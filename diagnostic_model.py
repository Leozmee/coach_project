# scripts/diagnostic_model.py - Diagnostic complet pour identifier le probl√®me

import os
import sys
import logging
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

def setup_logging():
    """Configure le logging pour le diagnostic"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def check_model_path():
    """V√©rifie le chemin du mod√®le"""
    logger = logging.getLogger(__name__)
    
    # Chemins √† v√©rifier
    possible_paths = [
        "./models/coach-sportif-french",
        "models/coach-sportif-french",
        "./coach-sportif-french",
        "coach-sportif-french"
    ]
    
    logger.info("üîç V√©rification des chemins du mod√®le...")
    
    for path in possible_paths:
        model_path = Path(path)
        logger.info(f"   V√©rification: {path}")
        
        if model_path.exists():
            logger.info(f"   ‚úÖ Trouv√©: {model_path.absolute()}")
            
            # V√©rifier les fichiers requis
            required_files = [
                "config.json",
                "pytorch_model.bin",
                "tokenizer.json",
                "tokenizer_config.json"
            ]
            
            missing_files = []
            for file in required_files:
                if not (model_path / file).exists():
                    missing_files.append(file)
            
            if missing_files:
                logger.warning(f"   ‚ö†Ô∏è  Fichiers manquants: {missing_files}")
            else:
                logger.info(f"   ‚úÖ Tous les fichiers requis pr√©sents")
                return model_path.absolute()
        else:
            logger.info(f"   ‚ùå Non trouv√©: {path}")
    
    return None

def test_model_loading(model_path):
    """Test de chargement du mod√®le"""
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"üöÄ Test de chargement du mod√®le: {model_path}")
        
        # Test tokenizer
        logger.info("üìù Chargement du tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(str(model_path))
        logger.info(f"   ‚úÖ Tokenizer charg√©. Vocabulaire: {len(tokenizer.vocab)}")
        
        # Configuration du pad token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info("   üîß Pad token configur√©")
        
        # Test mod√®le
        logger.info("ü§ñ Chargement du mod√®le...")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"   üì± Device: {device}")
        
        model = AutoModelForCausalLM.from_pretrained(
            str(model_path),
            torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
            low_cpu_mem_usage=True
        )
        model.to(device)
        model.eval()
        logger.info(f"   ‚úÖ Mod√®le charg√© sur {device}")
        
        # Test de g√©n√©ration
        logger.info("üß™ Test de g√©n√©ration...")
        test_prompt = "Comment faire des pompes ?"
        inputs = tokenizer.encode(test_prompt, return_tensors='pt').to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = response[len(test_prompt):].strip()
        
        logger.info(f"   ‚úÖ Test r√©ussi!")
        logger.info(f"   üìù Prompt: {test_prompt}")
        logger.info(f"   ü§ñ R√©ponse: {generated_text[:100]}...")
        
        return True, None
        
    except Exception as e:
        logger.error(f"   ‚ùå Erreur: {e}")
        return False, str(e)

def check_dependencies():
    """V√©rifie les d√©pendances"""
    logger = logging.getLogger(__name__)
    logger.info("üì¶ V√©rification des d√©pendances...")
    
    required_packages = {
        'torch': 'torch',
        'transformers': 'transformers',
        'fastapi': 'fastapi',
        'uvicorn': 'uvicorn',
        'streamlit': 'streamlit'
    }
    
    missing = []
    for package, import_name in required_packages.items():
        try:
            __import__(import_name)
            logger.info(f"   ‚úÖ {package}")
        except ImportError:
            logger.error(f"   ‚ùå {package} - MANQUANT")
            missing.append(package)
    
    return missing

def check_api_imports():
    """V√©rifie les imports de l'API"""
    logger = logging.getLogger(__name__)
    logger.info("üîå V√©rification des imports API...")
    
    try:
        from api.fitness_service import FitnessCoachService
        logger.info("   ‚úÖ FitnessCoachService import√©")
        
        from api.config import get_settings
        logger.info("   ‚úÖ Configuration import√©e")
        
        from api.models import FitnessRequest, FitnessResponse
        logger.info("   ‚úÖ Mod√®les Pydantic import√©s")
        
        return True
    except Exception as e:
        logger.error(f"   ‚ùå Erreur import: {e}")
        return False

def generate_fix_script(model_path, error_info=None):
    """G√©n√®re un script de correction"""
    logger = logging.getLogger(__name__)
    logger.info("üîß G√©n√©ration du script de correction...")
    
    fix_script = f"""# scripts/fix_model.py - Script de correction automatique

import os
import sys
import shutil
from pathlib import Path

def fix_model_path():
    \"\"\"Corrige le chemin du mod√®le\"\"\"
    source_path = Path("{model_path}")
    target_path = Path("models/coach-sportif-french")
    
    print(f"üîß Correction du chemin du mod√®le...")
    print(f"   Source: {{source_path}}")
    print(f"   Cible: {{target_path}}")
    
    # Cr√©er le dossier models si n√©cessaire
    target_path.parent.mkdir(exist_ok=True)
    
    # Copier ou cr√©er un lien symbolique
    if source_path != target_path:
        if target_path.exists():
            print(f"   ‚ö†Ô∏è  Suppression de l'ancien mod√®le...")
            shutil.rmtree(target_path)
        
        print(f"   üìÅ Copie du mod√®le...")
        shutil.copytree(source_path, target_path)
        print(f"   ‚úÖ Mod√®le copi√©!")
    
    return target_path

def update_api_config():
    \"\"\"Met √† jour la configuration de l'API\"\"\"
    config_content = '''# api/config.py - Configuration corrig√©e

import os
from pathlib import Path
from functools import lru_cache
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    \"\"\"Configuration de l'API Fitness Coach\"\"\"
    
    # API
    api_host: str = "127.0.0.1"
    api_port: int = 8001
    debug: bool = True
    
    # Mod√®le - CHEMIN CORRIG√â
    model_path: str = "./models/coach-sportif-french"
    model_device: str = "auto"
    
    # RAG
    enable_rag: bool = True
    rag_top_k: int = 3
    
    # G√©n√©ration
    max_new_tokens: int = 150
    temperature: float = 0.7
    top_p: float = 0.9
    
    # Logs
    log_level: str = "INFO"
    
    class Config:
        env_file = ".env"
        case_sensitive = False

@lru_cache()
def get_settings() -> Settings:
    \"\"\"R√©cup√®re la configuration avec cache\"\"\"
    return Settings()
'''
    
    with open("api/config.py", "w", encoding="utf-8") as f:
        f.write(config_content)
    
    print("‚úÖ Configuration API mise √† jour")

if __name__ == "__main__":
    print("üîß CORRECTION AUTOMATIQUE DU MOD√àLE")
    print("=" * 40)
    
    # Corriger le chemin
    model_path = fix_model_path()
    
    # Mettre √† jour la config
    update_api_config()
    
    print(f"\\nüéâ CORRECTION TERMIN√âE!")
    print(f"   Mod√®le: {{model_path}}")
    print(f"\\nüìã PROCHAINES √âTAPES:")
    print(f"   1. Red√©marrer l'API: python scripts/start_api.py")
    print(f"   2. Tester: curl -s http://127.0.0.1:8001/health | jq '.model_loaded'")
"""
    
    with open("scripts/fix_model.py", "w", encoding="utf-8") as f:
        f.write(fix_script)
    
    logger.info("   ‚úÖ Script de correction cr√©√©: scripts/fix_model.py")

def main():
    """Diagnostic principal"""
    logger = setup_logging()
    
    print("üîç DIAGNOSTIC MOD√àLE DISTILGPT-2")
    print("=" * 40)
    
    # 1. V√©rifier les d√©pendances
    missing_deps = check_dependencies()
    if missing_deps:
        print(f"\n‚ùå D√âPENDANCES MANQUANTES: {missing_deps}")
        print("   Installer avec: pip install " + " ".join(missing_deps))
        return False
    
    # 2. V√©rifier les imports API
    if not check_api_imports():
        print(f"\n‚ùå PROBL√àME D'IMPORTS API")
        print("   V√©rifier la structure du projet")
        return False
    
    # 3. Trouver le mod√®le
    model_path = check_model_path()
    if not model_path:
        print(f"\n‚ùå MOD√àLE NON TROUV√â")
        print("   V√©rifier que votre mod√®le DistilGPT-2 est bien pr√©sent")
        print("   Utiliser: python scripts/deploy_model.py /path/to/your/model")
        return False
    
    # 4. Tester le chargement
    success, error = test_model_loading(model_path)
    
    if success:
        print(f"\n‚úÖ MOD√àLE FONCTIONNEL!")
        print(f"   Chemin: {model_path}")
        print(f"\nüìã SI L'API NE FONCTIONNE TOUJOURS PAS:")
        print(f"   1. Red√©marrer l'API: python scripts/start_api.py")
        print(f"   2. V√©rifier: curl -s http://127.0.0.1:8001/health | jq '.model_loaded'")
        return True
    else:
        print(f"\n‚ùå ERREUR DE CHARGEMENT:")
        print(f"   {error}")
        
        # G√©n√©rer script de correction
        generate_fix_script(model_path, error)
        print(f"\nüîß SCRIPT DE CORRECTION G√âN√âR√â:")
        print(f"   Ex√©cuter: python scripts/fix_model.py")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)