# diagnostic_model_fixed.py - Version corrig√©e sans erreur de syntaxe

import os
import sys
import logging
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

def setup_logging():
    """Configure le logging pour le diagnostic"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def check_model_path():
    """V√©rifie le chemin du mod√®le avec support SafeTensors"""
    logger = logging.getLogger(__name__)
    
    # Chemins √† v√©rifier
    possible_paths = [
        "./models/coach-sportif-french",
        "models/coach-sportif-french",
        "./coach-sportif-french",
        "coach-sportif-french"
    ]
    
    logger.info("üîç V√©rification des chemins du mod√®le...")
    
    for path in possible_paths:
        model_path = Path(path)
        logger.info(f"   V√©rification: {path}")
        
        if model_path.exists():
            logger.info(f"   ‚úÖ Trouv√©: {model_path.absolute()}")
            
            # V√©rifier les fichiers requis (formats multiples)
            required_files = ["config.json"]
            model_files = [
                "pytorch_model.bin",      # Format PyTorch classique
                "model.safetensors",      # Format SafeTensors
                "pytorch_model-00001-of-00001.bin"  # Mod√®les divis√©s
            ]
            
            # V√©rifier fichiers de base
            missing_base = []
            for file in required_files:
                if not (model_path / file).exists():
                    missing_base.append(file)
            
            # V√©rifier au moins un fichier de mod√®le
            model_file_found = None
            for model_file in model_files:
                if (model_path / model_file).exists():
                    model_file_found = model_file
                    break
            
            if missing_base:
                logger.warning(f"   ‚ö†Ô∏è  Fichiers de base manquants: {missing_base}")
                continue
            elif not model_file_found:
                logger.warning(f"   ‚ö†Ô∏è  Aucun fichier de mod√®le trouv√©")
                continue
            else:
                logger.info(f"   ‚úÖ Mod√®le trouv√©: {model_file_found}")
                logger.info(f"   ‚úÖ Tous les fichiers requis pr√©sents")
                return model_path.absolute()
        else:
            logger.info(f"   ‚ùå Non trouv√©: {path}")
    
    return None

def test_model_loading(model_path):
    """Test de chargement du mod√®le avec support SafeTensors"""
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"üöÄ Test de chargement du mod√®le: {model_path}")
        
        # Test tokenizer
        logger.info("üìù Chargement du tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(str(model_path))
        logger.info(f"   ‚úÖ Tokenizer charg√©. Vocabulaire: {len(tokenizer.vocab)}")
        
        # Configuration du pad token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info("   üîß Pad token configur√©")
        
        # Test mod√®le avec support SafeTensors
        logger.info("ü§ñ Chargement du mod√®le...")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"   üì± Device: {device}")
        
        # D√©tection automatique du format
        model_path_obj = Path(model_path)
        if (model_path_obj / "model.safetensors").exists():
            logger.info("   üîí Format d√©tect√©: SafeTensors")
        elif (model_path_obj / "pytorch_model.bin").exists():
            logger.info("   ‚ö° Format d√©tect√©: PyTorch")
        
        model = AutoModelForCausalLM.from_pretrained(
            str(model_path),
            torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
            low_cpu_mem_usage=True,
            # SafeTensors est support√© automatiquement par transformers
        )
        model.to(device)
        model.eval()
        logger.info(f"   ‚úÖ Mod√®le charg√© sur {device}")
        
        # Test de g√©n√©ration
        logger.info("üß™ Test de g√©n√©ration...")
        test_prompt = "Comment faire des pompes ?"
        inputs = tokenizer.encode(test_prompt, return_tensors='pt').to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = response[len(test_prompt):].strip()
        
        logger.info(f"   ‚úÖ Test de g√©n√©ration r√©ussi!")
        logger.info(f"   üìù Prompt: {test_prompt}")
        logger.info(f"   ü§ñ R√©ponse: {generated_text[:100]}...")
        
        return True, None
        
    except Exception as e:
        logger.error(f"   ‚ùå Erreur: {e}")
        return False, str(e)

def check_dependencies():
    """V√©rifie les d√©pendances"""
    logger = logging.getLogger(__name__)
    logger.info("üì¶ V√©rification des d√©pendances...")
    
    required_packages = {
        'torch': 'torch',
        'transformers': 'transformers',
        'fastapi': 'fastapi',
        'uvicorn': 'uvicorn',
        'streamlit': 'streamlit'
    }
    
    missing = []
    for package, import_name in required_packages.items():
        try:
            __import__(import_name)
            logger.info(f"   ‚úÖ {package}")
        except ImportError:
            logger.error(f"   ‚ùå {package} - MANQUANT")
            missing.append(package)
    
    return missing

def check_api_imports():
    """V√©rifie les imports de l'API"""
    logger = logging.getLogger(__name__)
    logger.info("üîå V√©rification des imports API...")
    
    try:
        from api.fitness_service import FitnessCoachService
        logger.info("   ‚úÖ FitnessCoachService import√©")
        
        from api.config import get_settings
        logger.info("   ‚úÖ Configuration import√©e")
        
        from api.models import FitnessRequest, FitnessResponse
        logger.info("   ‚úÖ Mod√®les Pydantic import√©s")
        
        return True
    except Exception as e:
        logger.error(f"   ‚ùå Erreur import: {e}")
        return False

def main():
    """Diagnostic principal avec support SafeTensors"""
    logger = setup_logging()
    
    print("üîç DIAGNOSTIC MOD√àLE DISTILGPT-2 (SafeTensors Support)")
    print("=" * 55)
    
    # 1. V√©rifier les d√©pendances
    missing_deps = check_dependencies()
    if missing_deps:
        print(f"\n‚ùå D√âPENDANCES MANQUANTES: {missing_deps}")
        print("   Installer avec: pip install " + " ".join(missing_deps))
        return False
    
    # 2. V√©rifier les imports API
    if not check_api_imports():
        print(f"\n‚ùå PROBL√àME D'IMPORTS API")
        print("   V√©rifier la structure du projet")
        return False
    
    # 3. Trouver le mod√®le (avec support SafeTensors)
    model_path = check_model_path()
    if not model_path:
        print(f"\n‚ùå MOD√àLE NON TROUV√â")
        print("   V√©rifier que votre mod√®le DistilGPT-2 est bien pr√©sent")
        return False
    
    # 4. Tester le chargement avec SafeTensors
    success, error = test_model_loading(model_path)
    
    if success:
        print(f"\nüéâ MOD√àLE SAFETENSORS FONCTIONNEL!")
        print(f"   Chemin: {model_path}")
        print(f"   Format: SafeTensors (s√©curis√©)")
        print(f"\nüìã PROCHAINES √âTAPES:")
        print(f"   1. Red√©marrer l'API: python scripts/start_api.py")
        print(f"   2. Tester: curl -s http://127.0.0.1:8001/health | jq '.model_loaded'")
        print("   3. Chat test: curl -X POST http://127.0.0.1:8001/chat \\")
        print("      -H 'Content-Type: application/json' \\")
        print("      -d '{\"message\": \"Salut!\"}' | jq '.model_used'")
        
        return True
    else:
        print(f"\n‚ùå ERREUR DE CHARGEMENT:")
        print(f"   {error}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)