import os
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger explicitement le .env
load_dotenv()

class LlamaService:
    def __init__(self):
        print("üîß Initialisation LlamaService avec token HF...")
        
        # R√©cup√©rer le token
        token = os.environ.get("HF_TOKEN")
        if token:
            print(f"‚úÖ Token trouv√©: {token[:10]}...")
        else:
            print("‚ùå Token HF non trouv√©!")
            raise ValueError("Token Hugging Face non trouv√©")
        
        try:
            self.client = InferenceClient(
                model="meta-llama/Llama-3.2-3B-Instruct",
                token=token,
            )
            print("‚úÖ Client InferenceClient cr√©√©")
        except Exception as e:
            print(f"‚ùå Erreur cr√©ation client: {e}")
            raise
    
    def get_response(self, user_message, context_docs="", few_shot_examples=""):
        try:
            # Pr√©parer les messages pour le mode conversational
            messages = []
            
            # Message syst√®me avec le contexte
            system_message = f"""Tu es un coach sportif professionnel. R√©ponds en fran√ßais de mani√®re concise et pratique.

Utilise ces informations pour r√©pondre √† la question :
{context_docs}

Donne des conseils pr√©cis et pratiques."""

            messages.append({
                "role": "system", 
                "content": system_message
            })
            
            # Message utilisateur
            messages.append({
                "role": "user", 
                "content": user_message
            })

            print("üöÄ Envoi requ√™te √† Llama en mode conversational...")
            
            # Utiliser chat_completion au lieu de text_generation
            response = self.client.chat_completion(
                messages=messages,
                max_tokens=200,
                temperature=0.7,
            )
            
            # Extraire le contenu de la r√©ponse
            if response.choices and len(response.choices) > 0:
                return response.choices[0].message.content
            else:
                return "Aucune r√©ponse g√©n√©r√©e"
                
        except Exception as e:
            logger.error(f"Erreur d√©taill√©e: {e}")
            
            # Si l'erreur indique un probl√®me de mod√®le, essayer un autre mod√®le
            if "not supported" in str(e).lower():
                return self._try_alternative_model(user_message, context_docs)
            else:
                return f"Erreur : {str(e)}"
    
    def _try_alternative_model(self, user_message, context_docs):
        """Essaie un mod√®le alternatif si le premier ne fonctionne pas"""
        try:
            print("üîÑ Tentative avec un mod√®le alternatif...")
            
            # Essayer avec un mod√®le qui supporte text_generation
            alt_client = InferenceClient(
                model="microsoft/DialoGPT-medium",
                token=os.environ.get("HF_TOKEN"),
            )
            
            prompt = f"""Coach sportif: {context_docs}

Question: {user_message}
R√©ponse:"""

            response = alt_client.text_generation(
                prompt=prompt,
                max_new_tokens=200,
                temperature=0.7,
                return_full_text=False
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Erreur avec mod√®le alternatif: {e}")
            return f"""‚ùå Erreur technique avec l'IA

Mais voici une r√©ponse bas√©e sur le contexte trouv√© :

{context_docs}

Pour une r√©ponse plus personnalis√©e, essayez de vous connecter avec huggingface-cli login ou v√©rifiez votre configuration."""