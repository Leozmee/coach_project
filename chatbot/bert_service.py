from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from typing import List, Dict
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BertRAGService:
    def __init__(self):
        # Mod√®le BERT multilingue optimis√© pour la recherche s√©mantique
        try:
            print("Chargement du mod√®le BERT...")
            self.model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
            self.index = None
            self.documents = []
            self.embeddings = None
            print("‚úÖ Mod√®le BERT charg√© avec succ√®s")
        except Exception as e:
            logger.error(f"Erreur lors du chargement du mod√®le BERT: {e}")
            raise
        
    def load_fitness_data(self, documents: List[Dict]):
        """
        Charge les documents fitness et cr√©e l'index BERT
        """
        if not documents:
            logger.warning("Aucun document fourni")
            return
            
        try:
            self.documents = documents
            
            # Pr√©pare les textes pour l'encodage
            texts = []
            for doc in documents:
                # Combine titre + contenu pour une meilleure recherche
                title = doc.get('title', '')
                content = doc.get('content', '')
                text = f"{title} {content}".strip()
                texts.append(text)
            
            if not texts:
                logger.warning("Aucun texte valide trouv√© dans les documents")
                return
            
            # Encode avec BERT
            print("Encodage des documents avec BERT...")
            self.embeddings = self.model.encode(texts, show_progress_bar=True)
            
            # Cr√©e l'index FAISS pour la recherche rapide
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # Inner Product pour similarit√©
            
            # Normalise les embeddings pour la similarit√© cosinus
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings)
            
            print(f"‚úÖ Index BERT cr√©√© avec {len(documents)} documents")
            
        except Exception as e:
            logger.error(f"Erreur lors du chargement des donn√©es: {e}")
            raise
    
    def search_relevant_docs(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        Recherche les documents les plus pertinents pour une question
        """
        if self.index is None:
            logger.warning("Index BERT non initialis√©")
            return []
            
        if not query.strip():
            logger.warning("Query vide")
            return []
        
        try:
            # Encode la question avec BERT
            query_embedding = self.model.encode([query])
            faiss.normalize_L2(query_embedding)
            
            # Recherche dans l'index
            scores, indices = self.index.search(query_embedding, min(top_k, len(self.documents)))
            
            # Retourne les documents avec scores
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if 0 <= idx < len(self.documents):
                    doc = self.documents[idx].copy()
                    doc['relevance_score'] = float(score)
                    doc['rank'] = i + 1
                    results.append(doc)
            
            logger.info(f"Trouv√© {len(results)} documents pertinents pour: '{query}'")
            return results
            
        except Exception as e:
            logger.error(f"Erreur lors de la recherche: {e}")
            return []
    
    def format_context_for_llama(self, relevant_docs: List[Dict]) -> str:
        """
        Formate les documents trouv√©s pour le prompt Llama
        """
        if not relevant_docs:
            return "Aucun contexte sp√©cifique trouv√©."
        
        context = "Informations pertinentes :\n\n"
        for doc in relevant_docs:
            title = doc.get('title', 'Document')
            content = doc.get('content', '')
            score = doc.get('relevance_score', 0)
            
            context += f"üìã {title}\n"
            context += f"{content}\n"
            context += f"(Pertinence: {score:.2f})\n\n"
        
        return context